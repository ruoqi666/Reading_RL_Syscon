# SysCon reinforcement learning reading group

A weekly reinforcement learning reading group. We take turns picking a paper to read each week, which we then discuss together. Each session is 1h. Sessions are typically on Fridays, in room 103253. 

We also communicate in #reading_rl channel on the SysCon slack.

Contact person: Ruoqi Zhang (ruoqi.zhang@it.uu.se)

## Paper List 2023

| Week | Paper                                                        | Venue                  | Available Code                                               |
| ---- | ------------------------------------------------------------ | ---------------------- | ------------------------------------------------------------ |
| 24   | [Efficient Deep Reinforcement Learning Requires Regulating Overfitting](https://arxiv.org/abs/2304.10466) | ICLR 2023              |                                                              |
| 22   | [Planning with Diffusion for Flexible Behavior Synthesis](https://proceedings.mlr.press/v162/janner22a/janner22a.pdf) | ICML 2022              |                                                              |
| 21   | [Explainable Reinforcement Learning through a Causal Lens](https://ojs.aaai.org/index.php/AAAI/article/view/5631) | AAAI 2020              |                                                              |
| 19   | [EDGI: EQUIVARIANT DIFFUSION FOR PLANNING WITH EMBODIED AGENTS](https://openreview.net/pdf?id=OrbWCpidbt) | ICLR 2023 workshop RRL |                                                              |
| 18   | [DOPE: Doubly Optimistic and Pessimistic Exploration for Safe Reinforcement Learning ](https://nips.cc/Conferences/2022/ScheduleMultitrack?event=53063) | Neurips 2022           | [Official Code](https://github.com/archanabura/DOPE-DoublyOptimisticPessimisticExploration) |
| 17   | [Safe Reinforcement Learning With Stability Guarantee for Motion Planning of Autonomous Vehicles](https://ieeexplore.ieee.org/document/9478933) | IEEE 2021              |                                                              |
| 15   | [Information-Theoretic Regret Bounds for Gaussian Process Optimization in the Bandit Setting](https://ieeexplore.ieee.org/abstract/document/6138914) | IEEE 2012              |                                                              |
| 14   | [Safe Learning in Robotics: From Learning-Based Control to Safe Reinforcement Learning](https://www.ri.cmu.edu/pub_files/pub4/bagnell_james_2003_2/bagnell_james_2003_2.pdf) | Annual Review 2022     |                                                              |
| 12   | [When to Update Your Model: Constrained Model-based Reinforcement Learning](https://arxiv.org/abs/2210.08349) | NeurIPS 2022           |                                                              |
| 10   | [Neural Dynamic Policies for End-to-End Sensorimotor Learning](https://proceedings.neurips.cc/paper/2020/file/354ac345fd8c6d7ef634d9a8e3d47b83-Paper.pdf) | Neurips 2020           | [Official Code](https://github.com/shikharbahl/neural-dynamic-policies) |
| 8    | [Risk-Averse Bayes-Adaptive Reinforcement Learning](https://papers.nips.cc/paper/2021/hash/08f90c1a417155361a5c4b8d297e0d78-Abstract.html) | NeurIPS 2021           |                                                              |
| 7    | [Learning Latent Dynamics for Planning from Pixels](https://arxiv.org/abs/1811.04551) | ICML 2019              | [Official Code](https://github.com/google-research/planet)   |
| 5    | [Mastering Diverse Domains through World Models](https://arxiv.org/abs/2301.04104) | Arxiv 2023             | [Official Code](https://danijar.com/project/dreamerv3/)      |
| 4    | [Curiosity in Hindsight](https://openreview.net/forum?id=24_tkDqa2oL) | Neurips 2022           |                                                              |
| 3    | [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347) | Arxiv 2017             | [Stable Baselines 3](https://github.com/DLR-RM/stable-baselines3) |
| 2    | [Model-Predictive Planning via Cross-Entropy and Gradient-Based Optimization](https://people.eecs.berkeley.edu/~brecht/l4dc2020/papers/bharadhwaj20.pdf) | L4DC 2020              | [Official Code](https://github.com/homangab/gradcem)         |
| 1    | [Decision Transformer: Reinforcement Learning via Sequence Modeling](https://proceedings.neurips.cc/paper/2021/hash/7f489f642a0ddb10272b5c31057f0663-Abstract.html) | NeurIPS 2021           | [Official Code](https://github.com/kzl/decision-transformer) |

## Paper List 2022

| Week | Paper                                                        | Venue        | Available Code                                             |
| ---- | ------------------------------------------------------------ | ------------ | ---------------------------------------------------------- |
| 51   | [Temporal Difference Learning for Model Predictive Control](https://proceedings.mlr.press/v162/hansen22a.html) | ICML 2022    | [Official Code](https://github.com/nicklashansen/tdmpc)    |
| 50   | [Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://proceedings.mlr.press/v80/haarnoja18b.html) | ICML 2018    | [Official Code](https://github.com/haarnoja/sac)           |
| 49   | [Reinforcement Learning with Deep Energy-Based Policies](https://proceedings.mlr.press/v70/haarnoja17a.html) | PMLR 2017    | [Official Code](https://github.com/haarnoja/softqlearning) |
| 48   | [Conservative Q-Learning for Offline Reinforcement Learning](https://proceedings.neurips.cc/paper/2020/hash/0d2b2061826a5df3221116a5085a6052-Abstract.html) | NeurIPS 2020 | [Official Code](https://github.com/aviralkumar2907/CQL)    |
